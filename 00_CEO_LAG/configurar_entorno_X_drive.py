#!/usr/bin/env python3
"""
ğŸ”§ CONFIGURADOR DE ENTORNO X:\ - VHQ_LAG HÃBRIDO
Configura Ollama para usar disco X:\ en lugar de C:\ del sistema
Fecha: 27 de Junio 2025
"""

import os
import sys
import subprocess
from pathlib import Path

def print_header(title):
    print(f"\n{'='*70}")
    print(f"ğŸ”§ {title}")
    print(f"{'='*70}")

def check_ollama_status():
    """Verifica el estado actual de Ollama"""
    try:
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            print("âœ… Ollama funcionando correctamente")
            if result.stdout.strip():
                print("ğŸ“‹ Modelos ya instalados:")
                print(result.stdout)
            else:
                print("ğŸ“‹ No hay modelos instalados aÃºn")
            return True
        else:
            print("âš ï¸  Ollama instalado pero no hay modelos")
            return True
    except FileNotFoundError:
        print("âŒ Ollama no encontrado en PATH")
        return False
    except Exception as e:
        print(f"âš ï¸  Error verificando Ollama: {e}")
        return True  # Asumir que estÃ¡ disponible

def create_directory_structure():
    """Crea estructura de directorios en X:\ drive"""
    print_header("CREANDO ESTRUCTURA DE DIRECTORIOS")
    
    base_path = Path("X:/VHQ_LAG")
    directories = [
        "ollama_models",      # Modelos de Ollama
        "ollama_data",        # Datos temporales de Ollama
        "system_logs",        # Logs del sistema
        "checkpoints",        # Checkpoints de agentes
        "sessions_output",    # Output de sesiones colaborativas
        "model_cache"         # Cache de modelos
    ]
    
    for dir_name in directories:
        dir_path = base_path / dir_name
        try:
            dir_path.mkdir(parents=True, exist_ok=True)
            print(f"âœ… {dir_path}")
        except Exception as e:
            print(f"âŒ Error creando {dir_path}: {e}")
            return False
    
    print("ğŸ¯ Estructura de directorios creada correctamente")
    return True

def configure_environment_variables():
    """Configura variables de entorno para Ollama"""
    print_header("CONFIGURANDO VARIABLES DE ENTORNO")
    
    # Variables de entorno para Ollama
    env_vars = {
        'OLLAMA_MODELS': 'X:\\VHQ_LAG\\ollama_models',
        'OLLAMA_HOST': '127.0.0.1:11434',  # Puerto por defecto
        'OLLAMA_KEEP_ALIVE': '5m',         # Mantener modelos 5 minutos
        'OLLAMA_MAX_LOADED_MODELS': '1',   # Solo un modelo cargado por vez
        'OLLAMA_NUM_PARALLEL': '1',        # Una inferencia por vez
        'OLLAMA_FLASH_ATTENTION': '1',     # OptimizaciÃ³n de memoria
        'OLLAMA_MAX_QUEUE': '512'          # Queue limitado
    }
    
    print("ğŸ“‹ Variables de entorno a configurar:")
    for var, value in env_vars.items():
        print(f"   {var} = {value}")
    
    # Configurar variables para la sesiÃ³n actual
    for var, value in env_vars.items():
        os.environ[var] = value
        print(f"âœ… {var} configurado para sesiÃ³n actual")
    
    # Crear script de configuraciÃ³n permanente
    create_env_setup_script(env_vars)
    
    return True

def create_env_setup_script(env_vars):
    """Crea script para configurar entorno permanentemente"""
    
    # Script de PowerShell
    ps_script = """# ğŸ”§ CONFIGURACIÃ“N ENTORNO VHQ_LAG HÃBRIDO
# Ejecutar: .\setup_env.ps1

Write-Host "ğŸ”§ Configurando entorno VHQ_LAG en X:\ drive..."

"""
    
    for var, value in env_vars.items():
        ps_script += f'[Environment]::SetEnvironmentVariable("{var}", "{value}", "User")\n'
        ps_script += f'$env:{var} = "{value}"\n'
        ps_script += f'Write-Host "âœ… {var} configurado"\n\n'
    
    ps_script += """
Write-Host "ğŸ¯ Variables de entorno configuradas correctamente"
Write-Host "âš ï¸  REINICIA PowerShell para que los cambios tomen efecto"
Write-Host "ğŸš€ DespuÃ©s ejecuta: python implementar_sistema_hibrido.py"
"""
    
    with open('setup_env.ps1', 'w', encoding='utf-8') as f:
        f.write(ps_script)
    
    # Script de Batch
    bat_script = """@echo off
REM ğŸ”§ CONFIGURACIÃ“N ENTORNO VHQ_LAG HÃBRIDO
REM Ejecutar como administrador para configuraciÃ³n del sistema

echo ğŸ”§ Configurando entorno VHQ_LAG en X:\ drive...

"""
    
    for var, value in env_vars.items():
        bat_script += f'setx {var} "{value}"\n'
        bat_script += f'set {var}={value}\n'
        bat_script += f'echo âœ… {var} configurado\n\n'
    
    bat_script += """
echo ğŸ¯ Variables de entorno configuradas correctamente
echo âš ï¸  REINICIA CMD/PowerShell para que los cambios tomen efecto
echo ğŸš€ DespuÃ©s ejecuta: python implementar_sistema_hibrido.py
pause
"""
    
    with open('setup_env.bat', 'w', encoding='utf-8') as f:
        f.write(bat_script)
    
    print("ğŸ“ Scripts de configuraciÃ³n creados:")
    print("   âš¡ setup_env.ps1 - Para PowerShell")
    print("   âš¡ setup_env.bat - Para CMD")

def stop_ollama_service():
    """Para el servicio de Ollama si estÃ¡ corriendo"""
    print_header("CONFIGURANDO SERVICIO OLLAMA")
    
    try:
        # Intentar parar Ollama elegantemente
        result = subprocess.run(['tasklist', '/FI', 'IMAGENAME eq ollama.exe'], 
                              capture_output=True, text=True)
        if 'ollama.exe' in result.stdout:
            print("ğŸ›‘ Ollama detectado en ejecuciÃ³n, parando...")
            subprocess.run(['taskkill', '/F', '/IM', 'ollama.exe'], 
                         capture_output=True)
            print("âœ… Ollama parado")
        else:
            print("â„¹ï¸  Ollama no estaba ejecutÃ¡ndose")
            
    except Exception as e:
        print(f"âš ï¸  No se pudo parar Ollama: {e}")
        print("ğŸ’¡ Puedes pararlo manualmente desde Task Manager")

def verify_configuration():
    """Verifica que la configuraciÃ³n sea correcta"""
    print_header("VERIFICANDO CONFIGURACIÃ“N")
    
    # Verificar directorios
    required_dirs = [
        "X:/VHQ_LAG/ollama_models",
        "X:/VHQ_LAG/ollama_data", 
        "X:/VHQ_LAG/checkpoints"
    ]
    
    for dir_path in required_dirs:
        if Path(dir_path).exists():
            print(f"âœ… {dir_path}")
        else:
            print(f"âŒ {dir_path} - FALTA")
            return False
    
    # Verificar variables de entorno actuales
    print("\nğŸ“‹ Variables de entorno configuradas:")
    required_vars = ['OLLAMA_MODELS', 'OLLAMA_HOST', 'OLLAMA_MAX_LOADED_MODELS']
    for var in required_vars:
        value = os.environ.get(var, 'NO CONFIGURADA')
        print(f"   {var} = {value}")
    
    print("\nğŸ¯ ConfiguraciÃ³n completada correctamente")
    return True

def create_ollama_restart_script():
    """Crea script para reiniciar Ollama con nueva configuraciÃ³n"""
    
    restart_script = """@echo off
REM ğŸš€ REINICIAR OLLAMA CON CONFIGURACIÃ“N X:\

echo ğŸ›‘ Parando Ollama si estÃ¡ ejecutÃ¡ndose...
taskkill /F /IM ollama.exe 2>nul

echo â³ Esperando 3 segundos...
timeout /t 3 /nobreak >nul

echo ğŸš€ Iniciando Ollama con configuraciÃ³n X:\...
start "" ollama serve

echo âœ… Ollama reiniciado con configuraciÃ³n X:\
echo ğŸ¯ Los modelos se almacenarÃ¡n en X:\VHQ_LAG\ollama_models\
echo âš¡ Para verificar: ollama list

pause
"""
    
    with open('restart_ollama.bat', 'w', encoding='utf-8') as f:
        f.write(restart_script)
    
    print("ğŸ“ Script de reinicio creado: restart_ollama.bat")

def main():
    print_header("CONFIGURADOR ENTORNO X:\ DRIVE - VHQ_LAG HÃBRIDO")
    print("ğŸ¯ Objetivo: Mover todo Ollama y modelos a disco X:\\")
    print("ğŸ’¡ Beneficio: No saturar disco C:\\ del sistema")
    print(f"ğŸ“‚ UbicaciÃ³n base: X:\\VHQ_LAG\\")
    
    # Verificar Ollama
    if not check_ollama_status():
        print("âŒ Ollama no disponible. Instalar desde: https://ollama.ai")
        return False
    
    # Crear estructura
    if not create_directory_structure():
        print("âŒ Error creando directorios")
        return False
    
    # Configurar variables
    if not configure_environment_variables():
        print("âŒ Error configurando variables")
        return False
    
    # Parar Ollama
    stop_ollama_service()
    
    # Crear scripts auxiliares
    create_ollama_restart_script()
    
    # Verificar
    if not verify_configuration():
        print("âŒ Error en verificaciÃ³n")
        return False
    
    # Instrucciones finales
    print_header("âœ… CONFIGURACIÃ“N COMPLETADA")
    print("ğŸ¯ Ollama configurado para usar X:\\ drive")
    print("ğŸ“‚ Modelos se guardarÃ¡n en: X:\\VHQ_LAG\\ollama_models\\")
    print("ğŸ’¾ Datos temporales en: X:\\VHQ_LAG\\ollama_data\\")
    print("ğŸ“‹ Checkpoints en: X:\\VHQ_LAG\\checkpoints\\")
    
    print("\nğŸ“‹ PRÃ“XIMOS PASOS:")
    print("1. ğŸ”„ Ejecutar: .\\setup_env.ps1 (PowerShell)")
    print("   O ejecutar: .\\setup_env.bat (CMD como admin)")
    print("2. ğŸš€ Ejecutar: .\\restart_ollama.bat")
    print("3. ğŸ§ª Verificar: ollama list")
    print("4. âš¡ Ejecutar: python implementar_sistema_hibrido.py")
    
    print("\nğŸ’¡ VENTAJAS LOGRADAS:")
    print("âœ… Disco C:\\ del sistema protegido")
    print("âœ… Todo VHQ_LAG en un solo lugar (X:\\)")
    print("âœ… FÃ¡cil backup y migraciÃ³n")
    print("âœ… Control total sobre ubicaciones")
    
    return True

if __name__ == "__main__":
    try:
        success = main()
        if not success:
            print("\nâŒ ConfiguraciÃ³n fallÃ³. Revisar errores arriba.")
        else:
            print("\nğŸ‰ Â¡ConfiguraciÃ³n X:\\ drive completada!")
    except KeyboardInterrupt:
        print("\nâš ï¸  ConfiguraciÃ³n cancelada por usuario")
    except Exception as e:
        print(f"\nâŒ Error inesperado: {e}") 